\chapter{Hardware Trojan Insertion/Detection Principle Approaches and Tools}
\section{Introduction}
\paragraph*{}d
In the contemporary landscape of electronic systems and integrated circuits, the increasing complexity and interconnectedness have given rise to unprecedented challenges in ensuring the security and trustworthiness of these critical components. One particularly insidious threat that has garnered significant attention is the insertion of Hardware Trojans (HTs) - clandestine modifications in the hardware design or manufacturing process that compromise the functionality, reliability, or security of the targeted system. As electronic devices play an integral role in our daily lives, ranging from critical infrastructure to personal communication devices, the detection and prevention of Hardware Trojans have become paramount in safeguarding the integrity of these systems.
\paragraph*{}
This chapter explores the sophisticated domain of Hardware Trojans, specifically focusing on the insertion and detection approaches employed in countering this sophisticated threat.The chapter checks not only the techniques employed by adversaries to stealthily introduce Trojans into hardware but also the evolving strategies and technologies designed to identify and mitigate these malicious insertions.Central to this exploration is a comprehensive examination of the software and hardware materials utilized in both the perpetration and defense against Hardware Trojans.
\section{Insertion Approaches}
\paragraph*{}
HT insertion is considered possible at any design phase of an integrated circuit since not all chip third-party suppliers are to be trusted. The first crucial stage in embedded systems design is to gather and analyze the product requirements and turn them into specifications. This phase is the most trusted since suspicious constraints are in the form of text, which eases the process of debugging and checking without any advanced test mechanism.
\paragraph*{}
Whereas, during the design phase, register-transfer level (RTL) is a design abstraction that models a synchronous digital circuit in terms of the flow of digital signals (data) between hardware registers and the logical operations performed on those signals. Register-transfer-level abstraction is used in hardware description languages (HDLs) like Verilog and VHDL to create high-level representations of a circuit, from which lower-level representations and ultimately actual wiring can be derived. For that, untrusted third-party IPs and codes might be used, which would put the entire design at risk.
\paragraph*{}
Logic synthesis is a process by which an abstract specification of desired circuit behavior, typically at register transfer level (RTL), is turned into a design implementation in terms of logic gates, typically by a computer program called a synthesis tool. Common examples of this process include the synthesis of designs specified in hardware description languages, including VHDL and Verilog. Some synthesis tools generate bitstreams for programmable logic devices such as PALs or FPGAs, while others target the creation of ASICs. Besides Place and route is composed of two steps: placement and routing. The first step, placement, involves deciding where to place all electronic components, circuitry, and logic elements in a generally limited amount of space.
\paragraph*{}
This is followed by routing, which determines the exact design of all the wires needed to connect the components. This step must implement all the desired connections while following the rules and limitations of the manufacturing process. At this level of design all related libraries, tools, standard cells, and third-party hard IPs are to be properly certified, so that this step of design would be considered trusted. 
\paragraph*{}
Verification step is intended to check that a product, service, or system meets a set of design specifications. In the development phase, verification procedures involve performing special tests to model or simulate a portion, or the entirety, of a product, service, or system, then performing a review or analysis of the modeling results. In the post-development phase, verification procedures involve regularly repeating tests devised specifically to ensure that the product, service, or system continues to meet the initial design requirements, specifications, and regulations as time progresses. It is a process that is used to evaluate whether a product, service, or system complies with regulations, specifications, or conditions imposed at the start of a development phase. In the contrary with the previous phases, verification step typically performed in the local foundry, which qualify it to be trusted since the use of certified tools and testbenches.
\paragraph*{}
At the fabrication phase, third-party mask shops have access to genuine stream files (GDSII, OASIS), which might be an easy access to system applications and mess with genuine design. The most common attack at the assembly and package phase is to modify authentic hardware components during chip integration and replace them by malicious ones.
\paragraph*{}
In the end, the post-silicon test and validation phase will be safer if it is performed locally in the concerned factory or by another fully certified company instead of outsourcing from an untrusted third-party one, and this phase is considered the last to detect Trojans before delivering the IC for deployment.
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{../Figures/Trojan_insetion}
	\caption{Hardware trojan insertion in circuit design phases}
	\label{fig:trojaninsetion}
\end{figure}

\paragraph*{}
To assess the risks associated with Hardware trojans (HTs), various studies, such as \cite{insert01} , have presented taxonomies. These taxonomies abstract different categories related to the architecture, effects, and insertion of Trojans in Integrated Circuits (ICs).
\subsection{Insertion phase}
This classification delineates various stages within the IC design process where a potential adversary might be situated. The subsequent analysis outlines the susceptibilities to Trojan insertion at each phase:
\paragraph*{}
\textbf{Specification:} An adversary could intentionally establish weak requirements for the system. This could compromise design reliability, rendering the device susceptible to the leakage of sensitive information.
\paragraph*{}
\textbf{Design:} Even if the entire design is conducted in-house, the use of untrusted tools, libraries, third-party IPs, and standard cells may adversely impact it. For example, untrusted tools might introduce additional circuitry to create backdoors in the authentic design. If any aspect of the design phase is outsourced, a Trojan could be directly incorporated into the hardware description files of the genuine circuit.
\paragraph*{}
\textbf{Fabrication:} A foundry, mask shop, or their staff members who are not trustworthy may gain entry to authentic circuit components, enabling them to anticipate the circuit's functioning and possible uses. This makes the design susceptible to the insertion or deletion of components. Furthermore, modifying the physical characteristics of the circuit, such as sizes and channel doping concentration levels, can significantly increase the susceptibility of the circuit to attacks based on faults.
\paragraph*{}
\textbf{Assembly and package:} The IC is enclosed in a protective case, and the packaged chip is assembled on a PCB with other hardware. An adversary may introduce malicious hardware components around the genuine design to induce malfunctions or increase leakages.
\paragraph*{}
\textbf{Post-silicon test:} During the testing phase, an adversary can no longer modify the genuine circuit structure. However, the test setup, programs, or reports may be altered to mask potential Trojan effects. Additionally, as the final step in the IC design process, this phase represents the last opportunity for original designers to detect Trojans before the deployment stage.
\subsection{ Abstraction Level}
\paragraph*{}
The abstraction level pertains to the potential manipulation of the design in the event that an adversary gains access to sensitive files across various abstraction levels. The ensuing examination highlights opportunities for HT insertion at each abstraction level.
\paragraph*{}
\textbf{System level}: A Hardware Trojan (HT) can involve modifications to functional specifications, protocols, interfaces, and constraints within the authentic design. If an adversary operates at the system level, they might introduce ambiguous specifications to gain control over confidential data transmitted by the manufactured device. For example, during the specification phase, an adversary could manipulate the specifications of true random number generators (TRNG), causing them to operate predictably under specific conditions known only to the HT owner. Such alterations have the potential to significantly compromise the reliability of secure systems relying on these architectures, enabling attackers to access sensitive information.
\paragraph*{}
\textbf{Development environment level:} Tools and scripts that are not trusted may contain concealed functionalities, causing designers to create circuits that are compromised by Trojans. Moreover, untrusted simulation tools and testbenches have the potential to obscure Hardware Trojan (HT) effects. Any untrustworthy third-party vendor could introduce Trojans at this particular level. 
\paragraph*{}
\textbf{Register-transfer level:} A Hardware Trojan (HT) can manifest as straightforward alterations in authentic Register-Transfer (RT) level codes or constraint files. An adversary may manipulate circuit functions to induce significant outcomes, such as failures in cryptographic blocks. Potential sources for HT insertion at this level include attackers during the design phase or an untrusted supplier of code.
\paragraph*{}
\textbf{Gate level:} The inclusion or removal of one or more gates within the initial netlist is classified as a gate-level Hardware Trojan (HT). The standard delay format (SDF) files, which encompass system timing data, can also be altered to modify timing checks, constraints, and delays, concealing the effects of the HT. Adversaries operating during the gate design phase and third-party vendors possess the capability to introduce Trojans at this specific level.
\paragraph*{}
\textbf{Transistor level:} The introduction of additional transistors can substantially raise leakages, providing attackers with insights into the internal states of security-focused circuits. Additionally, the incorporation of transistors may be employed to extend critical path delays, causing malfunctions in the circuit. Possible origins of Trojans at this level include adversaries during the design phase or the utilization of untrustworthy tools, libraries, and models.
\paragraph*{}
\textbf{Physical layout level:} The initial parameters of circuit components remain susceptible even following layout generation. An example is an attacker manipulating original masks, modifying transistor dimensions like lengths, widths, or channel doping concentrations. Additionally, resizing wires can cause malfunctions and increased leakages. Adversaries operating at both the design and fabrication levels, as well as third-party mask shops, have the capability to modify the original layout and introduce such Trojans.
\subsection{Some employed strategies in the insertion of HTs}
\paragraph*{}
In emulating the strategies employed by adversaries in the insertion of hardware Trojan horses (HTH), Alkabani and Koushanfar categorized the necessary components into three groups: trigger, storage, and driver. A trigger serves to activate the intended HTH, and following the trigger event, the ensuing action can be recorded in either memory or a sequential circuit. The driver is responsible for executing the action prompted by the trigger. Based on this classification, Alkabani and Koushanfar propose a systematic approach for embedding hardware Trojans into integrated circuits (ICs) through pre-synthesis manipulation of the circuit's structure \cite{4559059}. This model addresses trust concerns related to Intellectual Property (IP) cores, especially when multiple cores from various vendors are utilized. In an abstracted view of the design process, the Trojan designer formulates a high-level design description to ascertain the computation model of the circuit that can be represented by a finite-state machine (FSM).
\paragraph*{}
Tiago D. Perez and Samuel Pagliarini have presented a Hardware Trojan Insertion in Finalized Layouts \cite{9956883}. Their work provides the first-ever documentation of how effortlessly an HT can be incorporated into a finalized layout. This is achieved by introducing an insertion framework based on the engineering change order flow. To validate their findings, they have constructed an ASIC prototype using 65-nm CMOS technology, featuring four trojaned cryptocores. In each core, a side-channel HT is inserted with the goal of leaking the cryptokey over a power channel.
\paragraph*{}
Yu Liu et al. have presented a wireless cryptographic IC containing two hardware Trojans capable of leaking the encryption key \cite{7792718}. Using silicon measurements from 40 chips fabricated in Taiwan Semiconductor Manufacturing Company's (TSMC's) 0.35-$\mu$ m technology, they demonstrate the operation of two hardware Trojans. These Trojans are designed to leak the secret key of a wireless cryptographic integrated circuit (IC) that includes an Advanced Encryption Standard (AES) core and an ultrawideband (UWB) transmitter (TX). Their impact is carefully concealed within the transmission specification margins allowed for process variations. These hardware Trojans evade detection by production testing methods for both the digital and analog parts of the IC, and they do not violate the transmission protocol or any system-level specifications. However, an informed adversary who knows what to look for in the transmission power waveform can retrieve the 128-bit AES key, leaked with every 128-bit ciphertext block sent by the UWB TX.
\paragraph*{}
Exurville et al., have described the structure of created HTs and how they are inserted at layout level in FPGA \cite{7092492}. The attack scenario involves an untrusted ASIC foundry. When the tape-out database (GDS file) is received, the perpetrator introduces a Hardware Trojan (HT) before the fabrication process. To minimize the impact of the HT on the authentic circuit, it is crucial to insert the HT while preserving the original placement and routing of the target circuit. For simulating HT insertion on ASICs, maintaining identical placement and routing between the golden circuit and the HT-infected circuit on FPGAs is essential. Consequently, the only disparities between the two lie in the logic and interconnect used by the HT.
\paragraph*{}
To insert an HT without altering the routing, the following steps are executed within the Xilinx framework:
\begin{enumerate}
	\item Synthesize, translate, map, place and route the original circuit, which, in this case, is an AES-128 block cipher.
	\item Extract the Native Circuit Description (NCD) file containing all the circuit, placement, and routing details of the original circuit (the golden model).
	\item Utilize the FPGA Editor tool to open the NCD file and manually or through a script, insert the HT in unused LUTs and Slices of the FPGA.
	\item Generate bit files for both the original and infected circuits using FPGA Editor.
\end{enumerate}
\paragraph*{}
This method ensures that the placement and routing remain consistent in both the golden and HT-infected circuits, facilitating the proof-of-concept of the HT attack at the ASIC layout level, where the FPGA fabric is considered as an ASIC.
\paragraph*{}
Yang et al. have developed an Analog malicious hardware trojan \cite{7546493}. They demonstrate how a fabrication-time attacker can utilize analog circuits to create a hardware attack that is both small (requiring as little as one gate) and stealthy (needing an unlikely trigger sequence before affecting a chip's functionality). In the open spaces of an already placed and routed design, a circuit is constructed using capacitors to siphon charge from nearby wires during transitions between digital values. When the capacitors fully charge, they execute an attack that compels a victim flip-flop to assume a desired value. The attack is weaponized into a remotely-controllable privilege escalation by connecting the capacitor to a controllable wire and selecting a victim flip-flop that stores the privilege bit for their processor. This attack is implemented in an OR1200 processor, and a chip is fabricated as part of the process.
\paragraph*{}
Cruz et al. have presented A Machine Learning Based Automatic Hardware Trojan Attack Space Exploration and Benchmarking Framework \cite{cruz2022machine}. They introduce MIMIC, a pioneering machine learning-guided framework for automated Trojan insertion. This framework has the ability to generate a substantial and targeted set of valid Trojans for a given design by emulating the characteristics of a small group of known Trojans. While existing tools can automatically insert Trojan instances using fixed Trojan templates, they lack the capability to analyze known Trojan attacks to create new instances that precisely capture the threat model. MIMIC operates in two primary steps: (1) it examines the structural and functional features of existing Trojan populations in a multi-dimensional space to train machine learning models and generate numerous "virtual Trojans" for the specified design, (2) subsequently, it integrates them into the design by aligning their functional/structural properties with suitable nets of the internal logic structure. 
\section{Detection Techniques}
\paragraph*{}
Hardware Trojan detection is a critical aspect of ensuring the security and reliability of integrated circuits and electronic systems. The primary goal of hardware Trojan detection is to identify the presence of such malicious entities within a chip and prevent their adverse impact on system performance. Several detection techniques and methodologies have been developed to address the growing sophistication of Hardware Trojans.
\paragraph*{}
The methods essentially assess the deviations induced by Hardware Trojans (HTs) on the system's behavior or seek potential HT profiles. In order to achieve this, designers need to be familiar with at least one specific parameter from the authentic device or define a target HT model for detection. If the deviation observed in the assessed parameter of a design under Trojan test (DUTT) surpasses an acceptable margin, the DUTT is categorized as being infected by a Trojan. A scheme derived from earlier surveys and research outlines the primary classifications of testing methods employed for identifying Trojans Figure \ref{fig:classificationhtdetection}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{../Figures/classification_ht_detection}
	\caption{Classification of hardware Trojan Detection Techniques.}
	\label{fig:classificationhtdetection}
\end{figure}

\paragraph*{}
The majority of current methods focus on detecting Trojans in manufactured integrated circuits (ICs) and presume the presence of a gate-level golden netlist. There have been limited studies addressing Trojan detection at higher-level design descriptions, such as the register transfer level IP. It's important to highlight that there is currently no singular, universally effective technique that can be employed to detect all types of Trojans.
\paragraph*{}
Trojan detection methods can be categorized into two primary types: destructive and non-destructive \cite{tehranipoor2011introduction}. Destructive techniques, involve employing a subset of manufactured integrated circuits (ICs) that undergo de-metallization using Chemical Mechanical Polishing (CMP)\footnote{Chemical mechanical polishing (CMP) is a planarization technique that was developed for semiconductor applications in the late 1980s and early 1990s. During this period, the number of metal layers increased dramatically and device topographies began to exhibit features that inhibited conformal deposition and gap fill by photoresist, metal, and insulator films. For more info visit: https://www.mks.com/n/chemical-mechanical-polishing, accessed 04 February 2024.}, followed by Scanning Electron Microscope (SEM) image reconstruction and analysis \cite{8741031}. The non-destructive methods can be categorized into two primary types: non-invasive and invasive. Non-invasive techniques preserve the original design without any alterations, whereas invasive techniques involve modifying the design to incorporate specific features aimed at detecting Trojans.
\subsection{Invasive Trojan Detection Technique}
\paragraph*{}
The invasive methods for detecting Trojans can be further categorized into two groups: one focuses on preventing the insertion of Trojans during the design or fabrication of an integrated circuit (IC), while the other enables the detection of inserted Trojans through post-manufacturing testing.
\paragraph*{}
In \cite{9491099}, The proposed framework employs self-testing, advanced imaging, and image processing with machine learning to detect hardware Trojans inserted by untrusted foundries. They incorporate on-chip test structures with minimal power, delay, and silicon area overheads. The central aspect of the framework involves on-chip golden circuit design, allowing the generation of authentic samples for image-based Trojan detection through self-testing. A fundamental aspect of this framework involves the creation of the golden circuit (GC), which is designed to generate trusted on-chip cell images for training machine learning algorithms. This is accomplished by populating the remaining vacant spaces with logic cells from the original circuit design after the Place and Route process. These additional cells are interconnected in a tree structure to attain full test coverage. Due to their limited number, these cells can be easily verified through logic testing to ensure the absence of inserted Trojans. Once authenticated, these cells serve as on-chip golden cells, acting as training samples for the image analysis-based Trojan detection.
\paragraph*{}
In \cite{9130873}, An innovative tamper-resistant approach is introduced to capture the electromagnetic signature of integrated circuits, enhancing their security. In this proposed method, the unused metal and polysilicon layers are maximally employed as internal magnetic probes to monitor the device's signature, simultaneously restricting attackers' access to layout resources for routing Hardware Trojans. In the proposed approach, following the routing of the main circuit, the unused metal and polysilicon layers are filled with minimum feature wires. These wires can be placed in various ways, either randomly or with the metals configured as inductive sensors. Opting for inductive sensors offers distinct advantages over random distribution. These sensors serve as internal magnetic probes capable of capturing the induced signature of the chip when specific sections of the main circuit are activated. By the end of this process, no resources remain for connecting Trojan circuits. If an attacker attempts to modify the magnetic probes' design or alter the main circuit placement to gain access to resources for inserting and routing a Trojan, it will alter the IC's signature. Additionally, the probes undergo separate testing to identify any changes in their properties.
\paragraph*{}
In \cite{5224960}, A design method named VITAMIN is introduced, relying on the inversion of the supply voltage for alternate logic levels within an integrated circuit (IC). In this technique, the logic behavior of a gate, functioning with an inverted supply voltage, undergoes inversion during the test mode. Consequently, the activity of a seldom activated Trojan circuit is heightened, facilitating its detection through a comparison of the power profiles among different ICs.
\paragraph*{}
In \cite{8993719}, They propose a hardware Trojan triggered by threshold voltage, operating within the 0.45V to 0.998V threshold voltage range. It consumes ultra-low power (10.5nW) and maintains stealthiness, with an area overhead as low as 1.5\% for a 28 nm technology node. The hardware Trojan detection sub-scheme introduces a lightweight, threshold voltage-aware sensor with a detection sensitivity of 0.251mV/nA. Using fixed and dynamic ring oscillator-based sensor segments, the proposal includes precise measurements of frequency and delay variations in response to threshold voltage shifts in a PMOS transistor. Lastly, the FPGA security scheme is strengthened by online transistor dynamic scaling (OTDS) to address hardware Trojan impact through run-time tolerant circuitry capable of identifying critical gates with worst-case drain current degradation.
\subsection{Non-invasive Trojan Detection Technique}
\paragraph*{}
In non-invasive Trojan detection methods, Trojans are identified by comparing the behavior of the test IC with either the golden IC instance or a golden functional model. These techniques can be categorized into two primary types: run-time and test-time methods. Run-time techniques utilize an online monitoring system to identify suspicious activities during in-field operation, whereas test-time techniques focus on detecting Trojan-infected chips before their deployment.
\subsubsection{Run-time, non-invasive Trojan detection approaches:}
\paragraph*{}
Run-time Trojan detection techniques involve monitoring and analyzing system behavior in real-time to identify potential malicious activities or the presence of Trojans during active operation. Several techniques are employed for this purpose:
\paragraph*{}
In \cite{7300085}, the focus is on detecting advanced Hardware Trojans using a run-time detection approach. They aim to identify high-level and critical behavioral invariants, checking them during the circuit operation. The properties to be verified are described using Assertion and Property Specification Language (PSL). Subsequently, a Hardware Property Checker (HPC) is developed and integrated into the IC to verify these properties in real-time.
\paragraph*{}
In \cite{6881484}, High-level synthesis for run-time hardware Trojan detection and recovery has been presented, They suggest establishing design guidelines to aid in run-time Trojan detection and swift recovery by exploring the diversity of untrusted third-party IP cores. Using these design guidelines, they demonstrate an optimization strategy aimed at reducing the implementation cost, specifically in terms of the number of distinct IP cores utilized in the implementation.
\paragraph*{}
In \cite{9840090}, they have presented Partial Reconfiguration for Run-time Memory Faults and Hardware Trojan Attacks Detection. The solution primarily relies on a centralized security detection controller for partially reconfigurable inspection content and a static memory wrapper to manage access conflicts and buffer testing cells. They demonstrate that a field programmable gate array prototype of the proposed framework can achieve the detection of 16 memory faults and three types of Hardware Trojans with one reconfigurable partition. This results in a 12.7\% reduction in area and a 2.9\% decrease in power overhead compared to a static implementation.
\paragraph*{}
In \cite{10251968}, a Secure Run-Time Hardware Trojan Detection Using Lightweight Analytical Models has been presented. They concentrate on hardware Trojans affecting the processor's performance. Current advanced detectors employ complex machine-learning models that monitor hardware counters for anomalies. These models necessitate a dedicated off-chip processor and extensive training for each target processor. In this study, they present a lightweight solution that utilizes data from a single reference run to accurately identify whether a Trojan is impeding processor performance across various CPU configurations, eliminating the need for new profiles. To achieve this, they employ an analytical model based on the application's inherent microarchitecturally independent characteristics. These models predict expected microarchitectural events across different processor configurations without requiring reference values for each application-hardware configuration pair. By comparing predicted values to actual hardware events, one can promptly identify unexpected application slowdowns, which are key indicators of many hardware Trojans.
\subsubsection{Test-time, non-invasive Trojan detection approaches:}
\paragraph*{}
Test-time, non-invasive Trojan detection approaches refer to methods employed to identify potential hardware Trojans during the testing phase of integrated circuits without causing any physical or operational harm. These approaches focus on examining the characteristics and behavior of the circuits before deployment. There are two primary categories of testing approaches for detecting Trojans: those centered on logic testing and those focused on measuring side-channel parameters like power and delay. Test-time techniques offer the advantage of no hardware overhead, contrasting with run-time methods. However, a drawback is the need for a "golden" (Trojan-free) manufactured IC or functional model. Run-time methods often come with notable performance and power overhead, yet they serve as the ultimate defense, ensuring 100\% confidence in computed results. Here are some common techniques in this category:
\paragraph*{}
In \cite{9684502}, they have introduced a golden-free multidimensional self-referencing technique that utilizes side-channel signatures in both the time and frequency domains. They significantly expand Trojan coverage and enhance detection confidence. The article presents a fully automated detection framework with systematic methodologies for generating tests, extracting signatures, processing signals, calculating thresholds, and making decisions based on metrics. They effectively enable the synergistic self-referencing approach. Finally, they assess the proposed technique using a comprehensive hardware measurement setup involving 96 Trojan-inserted test chips.
\paragraph*{}
In \cite{s13638022021659}, The article introduces a technique based on ring oscillators to enhance both wire and net coverage. The circuit under test is partitioned into numerous blocks, each incorporating a ring oscillator for detecting hardware Trojans. Additionally, a path tracking algorithm is outlined to optimize path assignments.
\paragraph*{}
In \cite{8885273}, This study introduces a compact supervisory circuit that classifies the operation of a Bluetooth (BT) System on Chip (SoC) at low frequencies. It achieves this by monitoring the input power and the radio frequency (RF) output of the BT chip through an envelope detector. The concept involves cost-effective fabrication of an envelope detector, power supply current monitor, and classification algorithm on a customized low-frequency integrated circuit in a trusted legacy technology. When unexpected behavior is detected, the supervisory circuit has the capability to cut off power to the BT SoC. In this initial phase, they prototype the supervisory circuit using readily available components. Simple yet descriptive features are extracted from the RF output signal envelope. Subsequently, machine learning (ML) models are trained to classify various BT operation modes, such as BT advertising and transmit modes.
\paragraph*{}
In \cite{9137882}, they have performed System Level Hardware Trojan Detection Using Side-Channel Power Analysis and Machine Learning. This paper suggests the detection of Hardware Trojans at the system level in the AES-256 decryption algorithm implemented in the Atmel XMega Controller (Target Board). The approach combines side-channel power analysis and machine learning. The ChipWhisperer-Lite board is employed for power analysis. Power traces from both the golden algorithm (Hardware Trojan-free) and Hardware Trojan-infected algorithms are acquired and utilized to train the machine learning model following the 80/20 rule. The resulting machine learning model achieves an accuracy range of 97\%-100\% for all the inserted Trojans.
\paragraph*{}
The Side Channel Analysis method involves detecting Hardware Trojans by measuring circuit parameters and comparing them with the golden circuit (trojan-free circuit). In this approach, the detection relies on measuring the path delay caused by Hardware Trojans in a digital circuit. However, if the impact of the path delay is minimal, detection becomes challenging. In \cite{9393058} addresses two key aspects. Firstly, it aims to increase the path delay, and secondly, it explores an alternative method to overcome the challenge of implausible differences between delays in the golden and trojan-infected circuits. The entire experiment is carried out in Cadence, utilizing a library with a feature size of 45nm. For the first part, combined sweeping is employed. In the second part, it is demonstrated that trojans can be detected by measuring the slope of the observed delay. 
\subsection{Advantages/disadvantages of invasive/non-invasive detection techniques}
\paragraph*{}
\subsubsection{Invasive Hardware Trojan Detection Techniques:}
\paragraph*{}
\subsubsection{Advantages}
\begin{enumerate}
	\item \textbf{Higher Sensitivity:} Invasive techniques often involve physical modifications or probing, providing higher sensitivity to detect subtle hardware Trojans that may be missed by non-invasive methods.
	\item \textbf{Direct Physical Examination:} Invasive techniques allow for a direct physical examination of the integrated circuit, making it easier to identify and analyze modifications at the hardware level.
	\item \textbf{Potential for Root Cause Analysis:} Invasive methods can facilitate a more detailed analysis, aiding in identifying the root cause of a Trojan and understanding its behavior.
\end{enumerate}
\subsubsection{Disadvantages}
\begin{enumerate}
	\item \textbf{Destructive Nature:} Invasive methods can be destructive, rendering the tested hardware unusable. This is a significant drawback if post-analysis or forensic examination of the hardware is desired.
	\item \textbf{Costly and Time-Consuming:} Invasive techniques often require specialized equipment and expertise, making them more costly and time-consuming compared to non-invasive methods.
	\item\textbf{ Ethical and Legal Concerns:} The invasive nature of these techniques raises ethical and legal concerns, especially if the hardware being tested is valuable or if the testing involves proprietary technology.
\end{enumerate}
\subsubsection{Non-Invasive Hardware Trojan Detection Techniques:}
\paragraph*{}
\subsubsection{Advantages}
\begin{enumerate}
	\item \textbf{Preservation of Hardware:} Non-invasive methods preserve the integrity of the hardware being tested, allowing it to remain functional after the detection process.
	\item \textbf{Lower Cost:} Non-invasive techniques are generally more cost-effective as they do not require specialized equipment for physical alterations or probing.
	\item \textbf{Applicability to Operational Systems:} Non-invasive methods can be applied to operational systems without the need for disrupting their functionality, making them suitable for real-world scenarios.
\end{enumerate}
\subsubsection{Disadvantages}
\begin{enumerate}
	\item \textbf{Lower Sensitivity:} Non-invasive techniques may have lower sensitivity compared to invasive methods, potentially missing subtle or well-hidden hardware Trojans.
	\item\textbf{ Indirect Detection:} Non-invasive methods often rely on indirect indicators such as power consumption or electromagnetic emissions, which may not provide a complete picture of the hardware.
	\item \textbf{Potential for False Positives/Negatives:} Non-invasive techniques may be prone to false positives or negatives, especially if the Trojans are designed to evade detection by these methods.
	\item \textbf{Limited Insight into Trojan Behavior:} Non-invasive techniques may provide limited insight into the behavior of detected Trojans compared to invasive methods.
\end{enumerate}
\paragraph*{}
In practice, a combination of both invasive and non-invasive techniques may be employed to achieve a more comprehensive and accurate hardware Trojan detection strategy. The choice depends on factors such as the specific characteristics of the hardware, the desired level of sensitivity, ethical considerations, and the resources available for testing.

\section{Software and Hardware Materials in HT Mitigation}
\paragraph*{}
Mitigating hardware Trojans, which are malicious modifications or additions to electronic circuits during the manufacturing process, requires a combination of software and hardware materials. The principal validation methods for complex systems are
simulation, testing, deductive verification, and Formal verification (Model checking).
%\subsection{Software Materials:}
%\paragraph*{}
\subsection{Simulation and testing}
\paragraph*{}
Conducting experiments prior to field deployment is common in both simulation and testing processes. Simulation is executed on a system's abstraction or model, while testing involves the actual product. In both approaches, signals or inputs are typically introduced at specific points within the system, and the resulting outputs are observed at other points. Although these methods are often cost-effective in identifying numerous errors, thoroughly examining all potential interactions and pitfalls through simulation and testing is rarely possible.
\subsection{Deductive verification}
\paragraph*{}
Deductive verification methods employ axioms and proof rules to establish the accuracy of systems. Initially, these proofs were manually crafted. Over time, researchers recognized that software tools could be employed to propose different approaches for advancing from the current proof stage. Deductive verification offers the benefit of being applicable to the analysis of systems with infinite states. While there is some automation possible for this task, it is important to note that, even if the property under verification is valid, there is no restriction on the potential time or memory resources required to discover a proof.
\paragraph*{}
Deductive verification is a labor-intensive procedure that is typically carried out by individuals with expertise in logical reasoning. As a result, it is primarily employed in critical systems, particularly those with high sensitivity such as security protocols.
%In \cite{FormalVerifThesis}, An approach of using a formal verification method, the model checking, to verify whether a particular component of hardware design matches its specification has been presented.
%\paragraph*{}
\subsection{Formal Verification:}
\paragraph*{}
Formal verification techniques use mathematical methods to prove the correctness of hardware designs. They can help identify and eliminate potential security vulnerabilities, including those introduced by hardware Trojans. The primary techniques for validating hardware systems include simulation, testing, and formal verification. Both simulation and testing entail conducting experiments prior to the system's deployment in real-world settings. Simulation is carried out on the design of the hardware system, while testing is executed on its actual implementation. These approaches are frequently employed as economical means of identifying numerous errors. Nevertheless, achieving a comprehensive examination of all potential behaviors in hardware systems through simulation and testing methods is rarely possible.
\paragraph*{}
Formal verification is employed, specifically model checking, to assess if a hardware system meets specified properties. Unlike simulation and testing approaches, model checking enables the examination of the system's behavior for all conceivable inputs. The goal is to assist hardware designers in developing transparent and accurate systems with assured functionality. Organizations such as Cadence Design Systems, IBM, and Mentor Graphics create tools that facilitate what is known as functional verification, aiming for identical objectives. They employ the Assertion-Based Verification (ABV) methodology to promptly identify errors in hardware design.
\paragraph*{}
\subsubsection{Temporal Logic}
\paragraph*{}
Temporal logic is a formal system used in the field of computer science and formal verification to reason about and specify the temporal aspects of system behavior. Temporal logic provides a way to express statements and properties about the order and timing of events in a system over time. There are two main branches of temporal logic: Linear Temporal Logic (LTL) and Computation Tree Logic (CTL). Both are used to specify and verify temporal properties, but they have different approaches and applications.
\paragraph*{}
Formal specification comprises one or more characteristics that can be represented as formulas in either Linear Temporal Logic (LTL) or Computational Tree Logic (CTL).
\paragraph*{}
\begin{enumerate}
	\item \textbf{Linear Temporal Logic (LTL):} LTL deals with linear sequences of time, expressing properties about the future along a single path. It includes temporal operators such as "next" (X), "until" (U), "always" (G), and "eventually" (F). For example, the LTL formula G(p → Xq) states that "it is always the case that if p holds, then in the next state, q will eventually hold."
	\item \textbf{Computation Tree Logic (CTL):} CTL is designed to reason about branching time, where multiple possible paths of execution exist. CTL introduces existential and universal path quantifiers, allowing the specification of properties that hold along some or all possible paths. CTL includes operators such as "AX" (for "for all next"), "EX" (for "there exists next"), "AG" (for "always globally"), and "EF" (for "eventually in the future").
\end{enumerate}
\paragraph*{}
Temporal logic is commonly used in formal verification methods, especially in model checking, where it helps specify and verify system properties over time. Model checking tools use temporal logic formulas to automatically explore all possible states of a system and check whether the specified properties hold in each state or along each possible path of execution.
\paragraph*{}
The application of temporal logic extends beyond formal verification and is also used in areas like real-time systems, concurrent programming, and specification of reactive systems, where understanding and reasoning about the temporal aspects of system behavior are crucial.
\paragraph*{}
\subsubsection{Model Checking}
\paragraph*{}
Model checking is a formal verification technique used to automatically check whether a given system or model satisfies a set of specified properties. It involves systematically exploring the entire state space of a system to verify if certain desired properties hold or if there are any violations. Model of the system and its specification are formulated in some precise mathematical language. To this end, the problem is formulated as a task in logic, namely to check whether a structure satisfies a given logical formula. This general concept applies to many kinds of logic and many kinds of structures. A simple model-checking problem consists of verifying whether a formula in the propositional logic is satisfied by a given structure. Model checking is widely employed in various domains, including hardware and software systems, communication protocols, and concurrent systems.
\paragraph*{}
The model checker typically employs a thorough exploration of the finite state space of the system to ascertain the truth or falsity of a given specification, which represents a property of the system. If the system does not meet a specified property, the model checker generates a counterexample that illustrates an incorrect behavior. This problematic sequence offers valuable insights into comprehending the underlying cause of the failure, along with crucial hints for resolving the issue.
\paragraph*{}
With adequate resources, the model checker will invariably conclude with either an affirmative or negative response. Additionally, it can be executed through algorithms that demonstrate reasonable efficiency.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\linewidth]{../Figures/model_checker}
	\caption{Model checker}
	\label{fig:modelchecker}
\end{figure}
\pagebreak
\paragraph*{}
Applying model checking to a design consists of several tasks:
\begin{enumerate}
	\item \textbf{Modeling:} The initial step involves transforming the system's design into an abstract model that is compatible with a model checking tool. The abstract model aims to remove extraneous details from the design, and while automated conversion is feasible in certain instances, human guidance and assistance are often necessary, especially in cases where it is challenging to automate (such as in software design).
	\paragraph*{}
	The behavioral modeling describes how the circuit should behave.	For these reasons, behavioral modeling is considered highest abstraction level as compared to data-flow or structural models.
	The VHDL synthesizer tool decides the actual circuit implementation.
	The VHDL behavioral model is widely used in test bench design, since the test bench design doesn’t care about the hardware realization
	\item \textbf{Specification:} The specification is typically presented in a logical formalism, often employing temporal logic to articulate the progression of system behavior over time. Key concerns in specification include:
	\begin{itemize}
		\item \textbf{ Consistency:} Is the provided specification free of contradictions?
		\item \textbf{Completeness:} Does the provided specification encompass all the properties that the system is required to fulfill?\\
		These matters are not explicitly tackled within the realm of model checking.		
	\end{itemize} 
	
	\item \textbf{Verification:} In an ideal scenario, verification is entirely automated. Nevertheless, practical implementation frequently requires human involvement. An example of such manual intervention is the examination of verification outcomes. In instances of a negative result, the user receives an error trace, aiding the designer in pinpointing the source of the error. Addressing the error may involve modifying the system and reapplying the model checking algorithm. An error trace can also result from a false negative, that is from:
		\begin{itemize}
		\item incorrect modeling of the system.
		\item incorrect formalization of the specification.
		\item inconsistent specification.
		\paragraph*{}
		A final possibility is that the verification task will fail to terminate normally, due to the size of the model, which is too large to fit into the computer memory
				
	\end{itemize} 
	
\end{enumerate}

\subsection{Physically Unclonable Functions (PUFs)}
\paragraph*{}
PUFs stand for Physically Unclonable Functions. PUFs are electronic components or systems that leverage the inherent physical variations in manufacturing processes to create unique and unpredictable identifiers for individual devices. These identifiers are difficult to clone or replicate, providing a form of hardware-based security.
\paragraph*{}
Because of variations in the deep submicron manufacturing process, each transistor within an integrated circuit possesses slightly distinct physical characteristics. These variances result in minor yet quantifiable discrepancies in electronic properties like transistor threshold voltages and gain factor. As these process variations are beyond full control during manufacturing, the physical attributes of the devices cannot be duplicated or replicated.
\paragraph*{}
Leveraging these inherent variations, PUFs prove highly beneficial as distinctive identifiers for individual integrated circuits (ICs). Achieved through the IC's internal circuitry, these minute variations are translated into a digital pattern of 0s and 1s, exclusive to that particular chip and consistently reproducible. This pattern serves as a "silicon fingerprint," analogous to the biometric characteristics found in humans.
\paragraph*{}
The application of this technology spans from employing Physical Unclonable Functions for IoT security, capitalizing on its cost-effectiveness and adaptable implementation for significant advantages, to utilizing Physical Unclonable Functions in Aerospace \& Defense. This demonstrates the technology's capability to provide the utmost level of security.
\subsubsection{The Advantages of a Physical Unclonable Function}
\paragraph*{}
Devices, especially those integrated into the Internet of Things (IoT), necessitate keys for safeguarding their data, intellectual property (IP), and functionalities. These keys may be embedded onto the devices either by the device manufacturers, also known as OEMs, or at an earlier stage by a chip vendor. When chip vendors furnish pre-provisioned chips, they enhance the overall worth of the product offered to OEMs. Alternatively, if OEMs opt for self-provisioning, they usually acquire chips at a lower cost.
\paragraph*{}
Regardless of whether the duty of provisioning cryptographic keys falls on the chip vendor or the device manufacturer, it is always a non-trivial undertaking. Introducing confidential keys into chips demands a reliable factory, introduces additional expenses and intricacies to the manufacturing process, and imposes restrictions on flexibility. This intricacy can be circumvented by generating the keys internally within the chip, utilizing either an internal random number generator (RNG) or a PUF.
\paragraph*{}
Nevertheless, generating a key poses only one aspect of the challenge. This is because securely storing keys on devices is also far from straightforward. Storing secret keys in non-volatile memory (NVM) is not a viable option, as NVM is susceptible to hardware attacks. With hardware attacks that enable adversaries to access NVM content becoming more prevalent, relying on unprotected key storage becomes impractical. Thus, there is a demand for alternative secure key storage solutions. One possible approach involves incorporating a secure element into the device. However, introducing additional hardware also brings heightened complexity and cost. A silicon PUF, on the other hand, offers a secure means of storing cryptographic keys without the necessity of adding extra hardware.
\subsubsection{Typical Use Cases for a Physical Unclonable Function in IoT Devices}
\paragraph*{}
\begin{itemize}
	\item \textbf{Key Vault:} The most widely recognized application of PUF technology involves generating and safeguarding the cryptographic root key for a device. The cryptographic root key, produced by the PUF, eliminates the need for key injection and is impervious to duplication from one device to another. This is because the key is never stored; instead, it is reconstructed from the unique silicon fingerprint of the device each time it is required. Given that this fingerprint varies for every chip, there is no conceivable method for an attacker to replicate a key from one device to another.
	\item \textbf{Firmware IP Protection:} Imagine an IoT device holding sensitive information that demands protection—perhaps valuable intellectual property containing confidential trade secrets or measurement data that is either privacy-sensitive or crucial to the system. In such cases, the device necessitates a secure vault. Within this secure vault, any form of data can be securely stored and inherently linked to the device's hardware. Achieving this is straightforward with a PUF, wherein all sensitive data is encrypted using a key derived from the PUF root key.
	\item \textbf{Edge-to-Cloud Security:} Establishing a secure channel between an IoT device and the cloud, utilizing a public key infrastructure like a Transport Layer Security (TLS) connection with a cloud service, involves the exchange of certificates between the device and the cloud. These certificates serve to authenticate the entities to one another. Generating a certificate to authenticate a device entails creating a public/private key pair derived from the PUF fingerprint.
\end{itemize}
\subsubsection{PUF Processing Algorithms}
\paragraph*{}
As mentioned earlier, the implementation of Physically Unclonable Functions (PUFs) necessitates processing algorithms to transform the silicon fingerprint into a cryptographic root key. This is crucial because the silicon fingerprint exhibits slight variations across different measurements due to inherent process differences, and external factors like ambient conditions (e.g., temperature and power supply) also impact electronic properties. Consequently, an effective PUF implementation must convert this potentially noisy fingerprint into a stable and genuinely random sequence of 0s and 1s to qualify as a cryptographic key. To achieve this, most PUF implementations employ two main processes:
\begin{enumerate}
	\item Error correction, ensuring consistency in the derived key across multiple PUF measurements.
	\item Privacy amplification, converting the fingerprint into a completely random string.
\end{enumerate}
\subsubsection{Error Correction}
\paragraph*{}
Error correction methods employed in the reconstruction of cryptographic keys involve two distinct phases: enrollment and reconstruction. During the one-time enrollment phase, the PUF response is correlated with a codeword from an error-correcting code. Details of this correlation are retained in the activation code (AC), also known as "helper data." The AC is designed in a way that it imparts no information about the key, and it can be stored off-chip since it lacks sensitivity. Although the AC must be accessible by PUF algorithms, any alteration, whether malicious or not, will hinder key reconstruction. Importantly, the AC remains valid exclusively for the chip on which it was originally generated.
\paragraph*{}
Whenever the device requires the confidential PUF key, a fresh PUF measurement, complete with noise, is conducted. Subsequently, the PUF key, devoid of noise, is derived from the activation code (AC) and this newly obtained PUF response. This step is referred to as the reconstruction phase. Both the enrollment and reconstruction phases are depicted in Figure \ref{fig:pufalgorithm} .
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{../Figures/PUF_algorithm}
	\caption{Error correction techniques for cryptographic key reconstruction}
	\label{fig:pufalgorithm}
\end{figure}
\subsubsection{Privacy Amplification}
\paragraph*{}
Confidential keys offer security by virtue of their complete randomness and consequent unpredictability. While physical measurements like PUF responses exhibit a substantial degree of randomness, they often fall short of being uniformly random. Privacy amplification algorithms come into play to produce uniformly random keys. This is achieved, for instance, by hashing a substantial amount of data with ample entropy, resulting in a random string comprising 128 or 256 bits.
\subsubsection{What Are the Challenges When Implementing a Physical Unclonable Function?}
\paragraph*{}
If Physically Unclonable Functions (PUFs) truly serve as reliable anchors of trust for devices, one might wonder why every chip vendor and original equipment manufacturer (OEM) doesn't adopt their own PUF implementations. The reason is that discovering and commercializing new types of PUFs is a challenging task. Identifying chip elements exhibiting the required behavior to create a device fingerprint involves extensive research before the actual productization phase begins. The process entails conducting millions of measurements under diverse conditions, accounting for silicon aging, to define the parameters necessary for error correction and privacy amplification algorithms. Typically, the journey of productizing a novel PUF implementation demands years of dedicated research and development efforts.
\paragraph*{}
Furthermore, it's worth noting that numerous PUF implementations, including those already existing and available in the market, often necessitate substantial modifications to the chip hardware. The integration of such PUFs results in alterations to the manufacturing process, either by augmenting the number of masks essential for production or by mandating non-standard processing steps. This inevitably raises the overall cost of incorporating PUF technology into chips, thereby undermining the cost-effectiveness advantage associated with the use of PUFs, as discussed earlier.
\subsubsection{Typical CMOS Implementations of Physical Unclonable Functions}
\paragraph*{}
Fortunately, these challenges can be easily addressed by both chip vendors and OEMs through the utilization of standard CMOS implementations of PUFs, seamlessly integrating them into their devices without necessitating any alterations to the chip hardware. The two PUF examples outlined below only require IP licensing and can be deployed on existing hardware. \textbf{The SRAM PUF} is designed for IoT platforms, like microcontrollers, and can be incorporated as software IP by an OEM. Chip vendors looking to incorporate this PUF into their products can opt for either soft- or hardware-IP versions. \textbf{The Butterfly PUF} caters to FPGA platforms, commonly employed in military, government, and aerospace applications, and can be implemented within the programmable fabric.

\begin{itemize}
	\item \textbf{SRAM PUF:} The performance of an SRAM cell is influenced by the variance in the threshold voltages of its transistors. Even minimal differences in these voltages get magnified, driving the SRAM cell into one of its two stable states. Consequently, its PUF behavior is significantly more consistent than the inherent threshold voltages, establishing it as the simplest and most reliable method for employing transistor threshold voltages to construct an identifier.
	\item \textbf{Butterfly PUF}: The Butterfly PUF is built on the concept of establishing formations within the FPGA matrix that exhibit characteristics akin to an SRAM cell during the initialization phase. A Butterfly PUF cell is a cross-coupled bi-stable circuit that can be induced into an unstable state before stabilizing into one of the two possible stable states.
\end{itemize}
\paragraph*{}
A Physical Unclonable Function, abbreviated as PUF, stands as a highly beneficial security component for both chip vendors and OEMs. The cryptographic key generated and securely established by a PUF serves as a foundational element of trust for a device. It forms the basis for successful applications in safeguarding keys, data, intellectual property (IP), and establishing secure connections with the cloud or other devices.
\paragraph*{}
Discovering and commercializing a Physical Unclonable Function (PUF) in electronic circuits demands extensive research and development efforts spanning several years. Fortunately, there are existing PUF implementations that are readily accessible for utilization. Examples include SRAM PUFs and Butterfly PUFs, which do not necessitate extra hardware and are founded on standard CMOS processes, enabling seamless integration into chips and devices at a minimal cost. These silicon-proven PUFs effectively address the challenges of key provisioning for manufacturers, providing a robust foundation of trust for any device.
\subsubsection{A Review of Some Studies on Physical Unclonable Functions (PUFs)}
\paragraph*{}
The concept of Physical Unclonable Functions (PUFs) has been a focal point in numerous studies, reflecting a significant interest and scholarly attention. Researchers from various disciplines have delved into the exploration of PUFs, investigating their properties, applications, and security implications. The prevalence of studies on PUFs attests to their relevance in the realm of hardware security and cryptographic systems. These collective research efforts contribute to a comprehensive understanding of PUFs, highlighting their potential and challenges across a spectrum of contexts.
\paragraph*{}
 A complete review of physical unclonable functions (pufs) and its applications in iot environment has been presented In\cite{yadav2022review}. The topic of the paper is a comprehensive exploration of physical unclonable functions (PUFs). The paper delves into various studies on PUFs, advocating for their utilization over conventional security mechanisms and conducting a thorough comparison across multiple dimensions. It introduces the classification of PUFs into strong and weak categories. The paper further discusses the implementation of authentication schemes for nodes, servers, routers, and network gateways in a network communication scenario. It elucidates the communication procedure through an architectural framework. Addressing security challenges faced by smart devices due to potential attacks is a significant aspect covered. Lastly, the paper reviews emerging concepts and advancements in the field of physical unclonable functions.
 \paragraph*{}
 In \cite{https://doi.org/10.1049/iet-cds.2019.0175}, Physical unclonable function: architectures, applications and challenges for dependable security has been deeply discussed. The physical design of PUF is thought to be easily produced but challenging or nearly impossible to replicate due to variations in the manufacturing process. Despite this, a substantial community of analysts perceives that hardware-based PUFs have opened avenues for achieving reliable security. This research thoroughly examines the architecture, applications, requirements, and challenges of PUFs for security solutions. To present the literature comprehensively, the authors have introduced a taxonomy that categorizes PUFs into two main groups: non-silicon and silicon-based PUFs. Currently, there is no all-encompassing survey that systematically compares the usability of memory-based PUFs with analogue/mixed-signal-based PUFs, which are considered more suitable than their counterparts. Similarly, the study outlines network-specific application scenarios in wireless sensor networks, wireless body area networks, and the Internet of Things, categorizing PUFs as strong, weak, and controlled. Additionally, the authors identify potential limitations in PUF structures and outline open research challenges to achieve desired security levels.
 \paragraph*{}
 Strong PUFs are susceptible to machine learning and modeling attacks.A solution is suggested where the challenges of a Strong PUF are encrypted to eliminate the linear challenge-response correlation exploitable by potential attacks In\cite{8854387}. In this scenario, a Weak PUF with a ZeroBit Error Rate generates the encryption key, ensuring that each PUF instance exhibits a distinct, nonlinear correlation between challenges and responses. Two implementations of this solution are introduced, and their robustness against machine learning attacks is showcased. 
 \subsection{Hardware-based Encryption}
 \paragraph*{}
 Hardware-based encryption is a method of securing data through the use of dedicated cryptographic hardware components. In this approach, encryption and decryption processes are offloaded to specialized hardware modules, providing a higher level of security and often better performance compared to software-based encryption.
 \paragraph*{}
 Key characteristics of hardware-based encryption include:
 \begin{itemize}
 	\item \textbf{Dedicated Hardware Components:} Hardware-based encryption relies on specialized hardware components, such as cryptographic processors or integrated circuits, designed specifically for handling encryption algorithms.
 	\item \textbf{Accelerated Processing:} The dedicated hardware is optimized for cryptographic operations, leading to faster encryption and decryption compared to software-based approaches. This is particularly beneficial in scenarios where real-time processing or high-throughput is essential.
 	\item \textbf{Secure Key Storage:} Hardware-based encryption often includes secure key storage mechanisms, protecting cryptographic keys from unauthorized access or extraction. This helps in preventing key compromise, a critical aspect of maintaining data security.
 	\item \textbf{Tamper Resistance:} Hardware-based encryption solutions may incorporate tamper-resistant features, making it difficult for attackers to physically access or manipulate the cryptographic components. This enhances the overall security of the system.
 	\item \textbf{Random Number Generation:} Some hardware-based encryption systems include dedicated modules for random number generation, crucial for the generation of secure cryptographic keys and initialization vectors.
 	\item \textbf{Cryptographic Algorithms Support:} Hardware-based encryption supports a variety of cryptographic algorithms, including symmetric key algorithms (e.g., AES) and asymmetric key algorithms (e.g., RSA), allowing for flexibility in implementation.
 	\item \textbf{Integration with Secure Elements:} Hardware-based encryption is often integrated into secure elements or modules, providing a secure environment for sensitive operations and key management.
 \end{itemize}
 \paragraph*{}
Applications of hardware-based encryption can be found in various fields, including data storage devices (e.g., self-encrypting drives), network communication devices, and secure communication modules in embedded systems. This approach is valued for its ability to provide robust security measures, especially in situations where the protection of sensitive information is paramount.
\subsubsection{A Review of Some Studies on Hardware-based encryption}
\paragraph*{}
A resilient architecture for the hardware implementation of the advanced encryption standard (AES) algorithm is introduced In \cite{MASOUMI2019102371}, featuring high efficiency and resistance against power analysis attacks. By selecting an appropriate topology, the FPGA implementation's resource requirements for the AES algorithm have been minimized. Furthermore, an innovative approach, combining a randomized SBox with a modified Boolean masking technique, eliminates the correlation between the Hamming distance of sensitive data and the algorithm's power consumption on the target platform. The robustness of the proposed outer masking, a modified version of the existing first-order Boolean masking scheme, is assessed through Welch's t-test statistical analysis and experimental results. Additionally, the effectiveness of the internal randomization technique within the SBox module relies on randomization in the underlying composite field $GF(2^{4})^{2}$. 
\paragraph*{}
Scalable and Efficient Hardware Architectures for Authenticated Encryption in IoT Applications has been presented In \cite{9326396}. Three generic implementation strategies (unrolled, round-based, and serialized) are proposed for developing highly efficient hardware architectures. These methods are suitable for all authenticated encryption schemes and are characterized by their lightweight and swift nature, as opposed to traditional public key encryption methods. Ascon serves as an illustration of the three outlined strategies: 1) The unrolled architecture achieves throughputs of 766.9 Mb/s (Ascon-128) and 1389.2 Mb/s (Ascon-128a), making it suitable for high-throughput IoT applications. 2) The round-based architecture achieves TP-to-area ratios of 0.153 (Ascon-128) and 0.244 (Ascon-128a), surpassing state-of-the-art results by 73.8\% and 40.2\%, respectively. 3) A novel serialized implementation technique is introduced, processing the substitution-box (S-box) in a multiple-bit-per-cycle fashion, a departure from the conventional one-bit-per-cycle approach. The two-bits-per-clock-cycle implementation increases throughput by 230.8\% with only 36.8\% additional hardware area. These strategies enable scalability in the number of rounds (round-based) and bits-per-clock-cycle (serialized), addressing varying requirements in throughput and area, as demonstrated in smart city IoT applications.
\paragraph*{}
In \cite{https://doi.org/10.1002/dac.4211}, A novel and efficient strategy has been proposed to enhance the security of biometric models, specifically fingerprint templates, against potential attacks. The suggested design relies on the Vernam stream cipher, with a hardware-based key generator. The devised cryptosystem incorporates a multi-scroll chaotic system known for its extensive key space, capable of generating N×N grid multi-scroll attractors with favorable chaotic dynamic behavior. The hardware implementation involves describing the Euler method using VHDL. Experimental results on a Field-Programmable Gate Array (FPGA) confirm the efficacy of the developed architecture, achieving a well-balanced trade-off between hardware resources and performance. Furthermore, security analysis demonstrates the robustness of the designed encryption algorithm against statistical, brute force, and entropy attacks. Consequently, this solution emerges as a lightweight security measure, particularly beneficial in various embedded applications, especially for securing biometric authentication systems.
\subsection{Trusted Platform Module (TPM)}
\paragraph*{}
A Trusted Platform Module (TPM) is a specialized hardware component that provides a secure foundation for the establishment of trust in computing environments. It is designed to enhance the security of systems by providing a range of cryptographic functions and capabilities.
\paragraph*{}
The Trusted Platform Module (TPM) encompasses fundamental elements crucial for enhancing the security of computing environments. This specialized hardware component integrates cryptographic functions, secure storage, and a root of trust to establish and maintain system integrity. TPM facilitates secure boot processes, remote attestation, and the sealing/unsealing of sensitive data, providing hardware-based security resistant to various attacks. Standardized by organizations like the Trusted Computing Group, TPM plays a pivotal role in safeguarding systems through its key features:
\begin{itemize}
	\item \textbf{Cryptographic Functions:} TPMs have built-in cryptographic functions, such as generating and storing keys securely, performing digital signatures, and executing hash functions. These capabilities enable the TPM to support various security applications.
	\item \textbf{Secure Storage:} TPMs include a secure storage area, often referred to as the Platform Configuration Registers (PCR), where sensitive information like cryptographic keys and measurements of system components can be securely stored.
	\item \textbf{Root of Trust:} TPM serves as a root of trust, providing a secure starting point for system integrity. It helps in establishing and maintaining trustworthiness throughout the boot process and during system operation.
	\item \textbf{Secure Boot:} TPM can be utilized in a secure boot process to ensure that only authorized and unaltered software components are loaded during system initialization. This helps prevent the execution of malicious code during startup.
	\item \textbf{Remote Attestation:} TPM supports remote attestation, allowing a system to prove its integrity to a remote entity. This is particularly useful in scenarios where one system needs to verify the trustworthiness of another system in a network.
	\item \textbf{Sealing and Unsealing:} TPMs enable the sealing of data to a specific set of system states, ensuring that sensitive information can only be accessed when the system is in a predefined, trusted state. Unsealing can then be performed when the system is in the expected state.
	\item \textbf{Hardware-Based Security:} As a dedicated hardware component, TPM is resistant to many software-based attacks. It is tamper-resistant and provides a higher level of security compared to purely software-based solutions.
	\item \textbf{Standardized Specifications:} TPM specifications are standardized by organizations such as the Trusted Computing Group (TCG), ensuring interoperability and compatibility across different hardware and software platforms.
\end{itemize}
\paragraph*{}
TPMs are commonly found in various computing devices, including laptops, desktops, and servers. They play a crucial role in bolstering the security of systems by providing a foundation for secure boot processes, cryptographic operations, and protection of sensitive information.
\subsubsection{A Review of Some Studies on The Trusted Platform Module (TPM)}
\paragraph*{}
A model delineating the structure of a standard for Trusted Computing (TC), specifically focusing on the Trusted Platform Module (TPM), which is an architecture with numerous potential implementations \cite{10.1145/3424771.3424781}. The architecture of TPM includes cryptographic functions that verify the utilization of authentic hardware and software platforms, along with defining operations for executing trusted tasks. Certain versions of TPM are designed exclusively for secure storage of private keys. Despite its potential, this technology is frequently underutilized. The first step of this methodology is presented in the form of a security pattern in this paper. An effort has been made to offer a reasonable amount of information in the pattern description, targeting the requirements of application designers. The pattern is depicted in an abstract form, independent of implementation details, yet articulated with sufficient detail and precision for the utilization by designers.
\paragraph*{}
In \cite{10.1145/3338511.3357348}, Hybrid Implementation of Trusted Platform Module (HTPM) has been presented. Since hardware-based Trusted Platform Module (TPM) encounters various inherent issues, including significantly low performance, susceptibility to off-chip security vulnerabilities, and a lack of responsiveness in incident handling. As we approach the era of Quantum computing, it becomes imperative to offer cryptography functions that are Quantum-Resistant (QR) without compromising performance. A TPM solution based on software delivers superior performance, on-chip security, and agility in incident response. Nevertheless, it lacks hardware-backed protection and essential features such as secure key storage, resilience against side-channel attacks, and genuine random number generation, among other capabilities. The HTPM operates as a completely dual-mode TPM, allowing end-users complete autonomy to select either a hardware TPM mode or a software TPM mode based on their requirements. In this study they have conducted and furnish a comprehensive risk analysis of the proposed HTPM to demonstrate the most effective ways to address security challenges in implementing the HTPM. Lastly, they have presented a performance analysis of their proposition to showcase significant enhancements in cryptographic operations.
\paragraph*{}
The growing frequency of cyber threats targeting Supervisory Control and Data Acquisition (SCADA) and automation systems within the Industrial Internet of Things (IIoT) and Industry 4.0 era has sparked apprehensions regarding the imperative to safeguard critical infrastructures and manufacturing facilities. The shift towards increased interconnection and interoperability has heightened the susceptibility of these systems. This vulnerability is particularly pronounced in the presence of widely used legacy standard protocols, which expose data to external networks. The objective of the paper \cite{s19194191} is to demonstrate the benefits of incorporating Trusted Platform Modules (TPMs) into automation/SCADA systems, emphasizing security enhancements. Two approaches are suggested to verify the authenticity of transmitted messages. The study also provides measurements regarding the additional time latency introduced by the proposed concept.
\subsection{Dual and Redundant Hardware Designs}
\paragraph*{}
Dual and redundant hardware designs refer to an approach in system architecture where critical components or subsystems are duplicated, and there is a built-in redundancy to ensure continued operation in the event of a hardware failure. This redundancy is implemented to enhance system reliability, availability, and fault tolerance. In a dual and redundant hardware design, if one component fails, the redundant counterpart can seamlessly take over, minimizing downtime and maintaining system functionality. This approach is commonly employed in safety-critical systems, mission-critical applications, and industries where uninterrupted operation is crucial. The redundancy provides a safeguard against hardware failures and contributes to overall system resilience.
\paragraph*{}
Implementing redundant hardware designs or dual-die configurations can provide a backup mechanism that helps mitigate the impact of a hardware Trojan by allowing the system to switch to a trusted component. Here are some key features of this technique:
\begin{itemize}
	\item \textbf{Fault Tolerance:} Dual and redundant hardware designs are implemented to enhance fault tolerance. If one component fails, the redundant counterpart can seamlessly take over, ensuring continued operation.
	\item \textbf{Reliability:} The redundancy in hardware components contributes to improved system reliability. The likelihood of a complete system failure is reduced, as there are backup components ready to assume control in case of a failure.
	\item \textbf{Availability:} By having duplicate hardware components, dual and redundant designs contribute to increased system availability. The system can continue functioning even during hardware failures, minimizing downtime.
	\item \textbf{Hot Swapping:} Some designs allow for "hot swapping" of components, meaning that faulty hardware can be replaced or repaired without shutting down the entire system. This is crucial for maintaining continuous operation.
	\item \textbf{Safety-Critical Applications:} Dual and redundant hardware designs are often used in safety-critical applications, such as aviation, medical devices, and nuclear systems, where system failure could have severe consequences.
	\item \textbf{Automatic Failover:} The system is designed to automatically switch to the redundant hardware if a failure is detected, without requiring manual intervention. This automatic failover mechanism enhances system responsiveness.
	\item\textbf{ Scalability:} Depending on the specific design, the redundancy can be scalable to meet the requirements of different applications. Additional redundant components can be added for increased reliability.
	\item \textbf{Monitoring and Diagnostics:} Dual and redundant hardware designs often include monitoring and diagnostic features to detect potential issues before they lead to a failure. This proactive approach helps in preventive maintenance.
	\item \textbf{Cost:} While the initial cost of implementing dual and redundant hardware designs may be higher, the potential cost savings from avoiding downtime and system failures can justify the investment, especially in critical applications.
	\item \textbf{Industry Standards:} Various industries may have specific standards and regulations for implementing dual and redundant hardware designs, especially in sectors where safety and reliability are paramount. Adherence to these standards is crucial for certification and compliance.
\end{itemize}
\paragraph*{}
the implementation of dual and redundant hardware designs stands as a key strategy to fortify critical systems against potential failures, ensuring a resilient and dependable operation that aligns with the stringent requirements of safety-critical industries.
\subsubsection{A Review of Some Studies on Dual and Redundant Hardware designs}
\paragraph*{}
An Architecture Design of Distributed Redundant Flight Control Computer Based on Time-Triggered Buses for UAVs have been presented In \cite{9201531}. This architecture introduces novel design approaches for UAV flight control, including distributed fault-tolerant management, Byzantine fault-tolerant design utilizing dual-core self-monitoring, and an open/integrated method for processing and fusing information from multiple sensors airborne. Leveraging the characteristics of the redundant Flight Control Computer (FCC) based on time-triggered buses, a model for distributed task scheduling and communication is established. An optimal static scheduling and real-time analysis algorithm, grounded in a search tree, is then proposed for these distributed tasks. Subsequently, an analysis and validation of the real-time performance and reliability of the FCC are conducted. The results of the verification demonstrate that, in comparison to the centralized FCC architecture relying on the event-triggered mechanism, the suggested UAV FCC architecture exhibits superior task schedulability and system scalability. Furthermore, it demonstrates enhanced task reliability under equivalent redundancy configurations, indicating its potential to furnish a distributed, synchronized, fault-tolerant, and redundantly reconfigurable technology platform for future UAV FCCs.
\paragraph*{}
The paper \cite{Amin2022} introduces a Unified Fault-Tolerant Control System (UFTCS) designed for Air-Fuel Ratio (AFR) control in Spark Ignition (SI) Internal Combustion (IC) engines, incorporating advanced analytical and hardware redundancies. The analytical redundancy, denoted as the Hybrid Fault-Tolerant Control System (HFTCS), integrates both active and passive components. The active part employs Lookup Tables (LTs), while the passive part features a robust proportional feedback controller with a high-gain fuel throttle actuator. To address the critical issue of engine shutdown resulting from the simultaneous failure of any two sensors or a single actuator, an advanced hardware redundancy protocol, Modified Triple Modular Redundancy (MTMR), is proposed for sensors, and Dual Redundancy (DR) is suggested for actuators to prevent engine tripping. Simulation results using MATLAB/Simulink demonstrate the UFTCS's robustness to sensor faults under normal and noisy conditions. Probabilistic reliability analysis of various hardware redundancy schemes further validates the superior overall reliability of the UFTCS. Finally, a comparative analysis with existing AFR control systems highlights its enhanced performance.
\paragraph*{}
Safety-critical computing systems, found in domains like avionics or space, necessitate specific safety measures based on deployment criticality. One challenge encountered by these systems involves transient hardware failures. To address potential issues, a common approach is to introduce redundancy, such as utilizing two cores simultaneously running the same program. However, this redundancy alone does not mitigate all potential failures, like Common Cause Failures (CCF), where a single fault impacts both cores uniformly (e.g., a voltage droop). If both redundant cores share identical states during a fault, the possibility of CCF arises, as the fault can affect both cores in a similar manner. In \cite{9774540}, SafeDM is introduced, a Diversity Monitor in hardware that quantifies the diversity of each redundant processor to ensure that CCF will not be overlooked, all without the necessity of deploying lockstepped cores. Data and instruction diversity are computed separately by SafeDM, employing distinct techniques suitable for each case. Integration of SafeDM is carried out in a RISC-V FPGA space MPSoC from Cobham Gaisler, where its effectiveness is demonstrated through extensive benchmarking, incurring minimal area and power overheads. In summary, an effective hardware solution in SafeDM is provided for quantifying diversity in cores engaged in redundant execution.
\subsection{Anti-tamper Packaging}
\paragraph*{}
Hardware anti-tamper packaging involves the use of specialized materials, mechanisms, and technologies to safeguard electronic devices and components against unauthorized access, tampering, or reverse engineering. This is particularly important in applications where the security and integrity of the hardware are critical, such as in defense systems, secure communication devices, and other sensitive electronic equipment. Employing anti-tamper packaging techniques, such as coatings or encapsulation, can make it more challenging for attackers to physically access and tamper with the hardware. Here are some common features and strategies used in hardware anti-tamper packaging:
\begin{itemize}
	\item \textbf{Encapsulation and Potting:} Components or entire devices may be encapsulated or potted in a resilient material to make physical access difficult. This can prevent unauthorized individuals from tampering with or extracting sensitive information from the hardware.
	\item \textbf{Tamper-Evident Seals:} Special seals or coatings are applied to the hardware, and any attempt to breach the packaging results in visible damage or alteration. This provides a clear indication that tampering has occurred.
	\item \textbf{Anti-tamper Coatings:} Applying coatings that are resistant to probing, tampering, or chemical attacks can enhance the overall resistance of the hardware to unauthorized access.
	\item \textbf{Secure Enclosures:} Using robust and secure enclosures with complex locking mechanisms adds an additional layer of protection against physical tampering.
	\item \textbf{Self-Destructive Features:} Some anti-tamper packaging incorporates self-destructive features that trigger when tampering is detected, rendering the hardware inoperable or destroying sensitive data.
	\item \textbf{Intrusion Detection Systems:} Implementing sensors or systems that can detect physical tampering and trigger alerts or protective measures.
	\item \textbf{Unique Identifiers:} Embedding unique identifiers, such as serialized codes or cryptographic keys, into the hardware can help verify its authenticity and detect any attempts at tampering.
	\item \textbf{Secure Boot and Authentication:} Implementing secure boot processes and authentication mechanisms at the hardware level can prevent unauthorized access and ensure that only authenticated software can run.
	\item \textbf{Environmental Sensors:} Integrating sensors that monitor environmental conditions, such as temperature, humidity, or radiation, can help detect attempts to compromise the hardware.
	\item \textbf{Shielding Against Side-Channel Attacks:} Employing materials and techniques to protect against side-channel attacks, such as power analysis or electromagnetic analysis, which could be used to extract sensitive information.
\end{itemize}
\paragraph*{}
Anti-tampering design encompasses a range of methods aimed at thwarting, identifying, or addressing physical assaults on a chip. It stands as an essential facet of chip security, executed through physical layout, digital RTL, or a blend of both. Typically manifesting as a protective shield, anti-tamper designs combat diverse attack forms, like data scrambling during writing or identifying glitches in circuitry. Consequently, the implementation of anti-tampering design is paramount in safeguarding sensitive information against the three primary tampering attack categories: \textbf{invasive, semi-invasive, and non-invasive.}
\paragraph*{Invasive}
 attacks initiate with the dismantling of the chip packaging and elimination of the passivation layer. Subsequently, through physical scrutiny, hackers can acquire access to the circuitry designs and security expertise. Alternatively, they may manipulate signals on the exposed chip to induce erroneous behavior or disrupt regular operations.
 \paragraph*{Semi-invasive}
 attacks involve deliberately causing electrical malfunctions within a chip and subsequently observing the resulting consequences.
\paragraph*{Non-invasive}
attacks are passive in nature, where hackers observe the regular functioning of a chip without making alterations or directly influencing any aspect.
\paragraph*{}
Thus, anti-tampering design encompasses all the approaches to combat the diverse forms of tampering attacks.
\subsubsection{Anti-tampering Techniques}
\begin{enumerate}
	\item Tamper resistance depends on limiting the physical access to a chip, making tampering a more challenging and time-consuming task. Standard tamper-resistance methods include physical packaging technologies, metal shielding, inherent physical security, post-masking, and controlling access to prevent unauthorized entry. Furthermore, the security function "random dummy read" can prevent repetitive reads at the same address, incorporating address and data bus scrambling techniques to add complexity and bewilder potential hackers.
	\item Tamper detection involves the capability to identify and perceive tampering attempts. In any secure design, it is crucial to incorporate anti-tamper circuitry that conducts health checks and detects irregularities in power supply as well as instances of fault injection.
	\item Tamper response pertains to the actions taken upon detecting tampering. Examples of potential tamper responses encompass triggering alarm signals (interrupts), deactivating or disabling a device, and eliminating or erasing critical memory space.
	\item Tamper evidence involves generating visible indicators that persist when tampering takes place, enabling authorized personnel to determine whether tampering has occurred or not.
\end{enumerate}








%Mitigating hardware Trojans, which are malicious modifications or additions to electronic circuits during the manufacturing process, requires a combination of software and hardware materials. Here are some software and hardware materials commonly used in hardware Trojan mitigation:
%
%**Software Materials:**
%
%1. **Design Verification Tools:**
%- Advanced design verification tools are used to analyze hardware designs for potential vulnerabilities and detect any unauthorized modifications or Trojans. These tools help ensure the integrity of the design before fabrication.
%
%2. **Formal Verification:**
%- Formal verification techniques use mathematical methods to prove the correctness of hardware designs. They can help identify and eliminate potential security vulnerabilities, including those introduced by hardware Trojans.
%
%3. **Hardware Trojans Detection Software:**
%- Specialized software tools are available for detecting hardware Trojans in fabricated chips. These tools analyze chip behavior or perform side-channel analysis to identify suspicious patterns indicative of Trojan activity.
%
%4. **Secure Supply Chain Management Software:**
%- Supply chain management software helps track the flow of electronic components and verify their authenticity throughout the manufacturing process. It can help prevent the insertion of Trojans at various stages of production.
%
%5. **Security Patching and Updates:**
%- Regular updates to design tools, verification software, and security protocols are essential for addressing newly discovered vulnerabilities and mitigating potential risks associated with hardware Trojans.
%
%Hardware Materials:
%
%Physically Unclonable Functions (PUFs):
%
%PUFs generate unique identifiers based on physical characteristics of the hardware, making it difficult for attackers to clone or tamper with hardware components without detection.
%Hardware-based Encryption:
%
%Incorporating hardware-based encryption mechanisms can protect sensitive data from being intercepted or manipulated by hardware Trojans.
%Trusted Platform Module (TPM):
%
%TPM, in addition to its software applications, can also provide hardware-based security features that help ensure the integrity and authenticity of the system.
%Dual and Redundant Hardware Designs:
%
%Implementing redundant hardware designs or dual-die configurations can provide a backup mechanism that helps mitigate the impact of a hardware Trojan by allowing the system to switch to a trusted component.
%Formal Verification Techniques:
%
%Use formal methods and verification techniques during the design and manufacturing processes to rigorously analyze and validate the correctness of the hardware components, reducing the likelihood of Trojan insertion.
%Anti-tamper Packaging:
%
%Employing anti-tamper packaging techniques, such as coatings or encapsulation, can make it more challenging for attackers to physically access and tamper with the hardware.